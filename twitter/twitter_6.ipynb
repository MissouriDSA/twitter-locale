{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter: An Analysis in Linguistic Diversity\n",
    "\n",
    "## Part VI\n",
    "\n",
    "There are more tables than just the `tweet` and `job` table. Everytime there is a hashtag in a tweet, the `hashtag` table updates with a new row with a tweet id, character start location of the hashtag, and the hashtag itself (just the text, sans #). The tweet id maps to the `tweed_id_str` of the `tweet` table so that we can connect back to the original text or other features of the tweet. The character start location marks where in the sequence of letters the hashtag begins. Surprsingly, this is more meaningful than one might think as hashtags often hold different roles depending on where they are within the tweet. \n",
    "\n",
    "We are going to be continuing the analysis of twitter data but we are going to need to find some important characteristics pertaining to hashtags.\n",
    "\n",
    "Let's start off by find the most popular hashtags (Remember, we are putting a limit on the query for performance purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BE SURE TO RUN THIS CELL BEFORE ANY OF THE OTHER CELLS\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# query database\n",
    "statement = \"\"\"\n",
    "SELECT DISTINCT text, COUNT(*)\n",
    "FROM \n",
    "(SELECT text \n",
    "FROM twitter.hashtag\n",
    "LIMIT 10000) AS hashtag_text\n",
    "GROUP BY text\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "    \n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "pd.DataFrame(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, lets find the most popular hashtags for the city of Provo, Utah!\n",
    "\n",
    "We first need create a relationship between the `job` table and the `tweet` table and then from the `tweet` table to the `hashtag` table.\n",
    "\n",
    "Then we want to search the `job`.`description` for something like \"Provo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query database\n",
    "statement = \"\"\"\n",
    "SELECT DISTINCT lower(text), COUNT(*)\n",
    "FROM \n",
    "(SELECT h.text\n",
    "FROM twitter.hashtag h, twitter.tweet t, twitter.job j\n",
    "WHERE h.tweet_id = t.tweet_id_str AND t.job_id = j.job_id AND j.description LIKE 'Provo%'\n",
    "LIMIT 10000) AS hashtag_text\n",
    "GROUP BY lower(text)\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "    \n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "pd.DataFrame(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used `lower(text)` in our `group by`. Again, hashtags can be typed out different ways but they will all be the same hashtag. For example, `#GameOfThrones` is the same as `#gameofthrones`. \n",
    "\n",
    "Postgres also has some built in functions that make preprocesssing our data for particular purposes a lot simpler. Imagine that we wanted to count the most common words in the tweet text. We can do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = '''\n",
    "SELECT lower(word), COUNT(DISTINCT rn) AS num_rows\n",
    "FROM(\n",
    "    SELECT UNNEST(STRING_TO_ARRAY(text, ' ')) AS word,\n",
    "       ROW_NUMBER() OVER(ORDER BY text) AS rn\n",
    "    FROM \n",
    "        (SELECT text FROM\n",
    "        twitter.tweet LIMIT 10000) y\n",
    "        ) x\n",
    "GROUP BY lower(word)\n",
    "ORDER BY num_rows DESC'''\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "    \n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "pd.DataFrame(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things going on here so we will break it down piece by piece and then put the components back together again.\n",
    "\n",
    "```sql\n",
    "SELECT UNNEST(STRING_TO_ARRAY(text, ' ')) AS word,\n",
    "       ROW_NUMBER() OVER(ORDER BY text) AS rn\n",
    "FROM \n",
    "       (SELECT text FROM\n",
    "       twitter.tweet LIMIT 1000) y\n",
    "```\n",
    "\n",
    "So line by line:\n",
    "\n",
    "```sql\n",
    "SELECT UNNEST(STRING_TO_ARRAY(text, ' ')) AS word,\n",
    "```\n",
    "\n",
    "This is first going to turn the `text` column into an array where each word is an item in the array. It then `unnests` makes each word its own value in a row. We call this new column of words `word`. \n",
    "\n",
    "```sql\n",
    "ROW_NUMBER() OVER(ORDER BY text) AS rn\n",
    "```\n",
    "\n",
    "This column just creates a column of row numbers.\n",
    "\n",
    "Let's take a look at what this produces before the aggregations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = '''\n",
    "    SELECT UNNEST(STRING_TO_ARRAY(text, ' ')) AS word,\n",
    "       ROW_NUMBER() OVER(ORDER BY text) AS rn\n",
    "    FROM \n",
    "        (SELECT text FROM\n",
    "        twitter.tweet LIMIT 1000) y \n",
    "'''\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "    \n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "pd.DataFrame(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with just counting words is that some words don't hold a lot of meaning (or they aren't that interesting when doing analysis). These are words like 'I', 'your', 'is', etc... These are known as stop words. We can actually load in a list of stop words from the Natural Language Toolkit (`nltk`) library. Now, this list is tokenized and we haven't tokenized the text of our tweets, so some probably won't apply, but it will allow us to remove a lot of these stop words from the scope of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to integrate this list into our statement. If the word is in this list, we don't want it returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = '''\n",
    "SELECT lower(word), COUNT(DISTINCT rn) AS num_rows \n",
    "FROM\n",
    "(SELECT * FROM\n",
    "(SELECT UNNEST(STRING_TO_ARRAY(text, ' ')) AS word,\n",
    "       ROW_NUMBER() OVER(ORDER BY text) AS rn\n",
    "    FROM \n",
    "        (SELECT text FROM\n",
    "        twitter.tweet LIMIT 1000) y ) x) z\n",
    "        WHERE lower(word) NOT IN ('rt',{})\n",
    "    GROUP BY lower(word)\n",
    "ORDER BY num_rows DESC; \n",
    "'''.format(', '.join(map(\"'{}'\".format, stops)))\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "    \n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "pd.DataFrame(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this isn't perfect in terms of cleaning up our results, but it did remove quite a bit of unwanted information. It is important to keep in mind that Twitter is very messy, more so than other natural language documents. That is because it is informal and users are constrained to write tweets within 140 characters. This lends itself to some creative spelling. \n",
    "\n",
    "Also, notice how we removed 'rt' as well.\n",
    "\n",
    "Now we can also extract the most common words per hashtag to begin to get a sense of what people might be talking about when using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = '''\n",
    "SELECT lower(word), COUNT(DISTINCT rn) AS num_rows \n",
    "FROM\n",
    "(SELECT * FROM\n",
    "(SELECT UNNEST(STRING_TO_ARRAY(text, ' ')) AS word,\n",
    "       ROW_NUMBER() OVER(ORDER BY text) AS rn\n",
    "    FROM \n",
    "        (SELECT t.text FROM\n",
    "        twitter.tweet t, twitter.hashtag h\n",
    "        WHERE t.tweet_id_str = h.tweet_id AND lower(h.text) = 'tornado'\n",
    "        LIMIT 1000) y ) x) z\n",
    "        WHERE lower(word) NOT IN ('rt',{})\n",
    "    GROUP BY lower(word)\n",
    "ORDER BY num_rows DESC; \n",
    "'''.format(', '.join(map(\"'{}'\".format, stops)))\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "    \n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "pd.DataFrame(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to see what words most commonly occur with \"#tornado\" and the results make sense.  \n",
    "\n",
    "# <span style=\"background-color: #FFFF00\">YOUR TURN</span>\n",
    "\n",
    "Choose a hashtag and find the most common words from 10,000 tweets that use the hashtag. Do the results make some sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# --------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we created a very simple way of modeling topics in twitter according to hashtags using almost entirely Postgres. There are some more advance methods of topic modeling but this is a good start in order to grasp what people are talking about most commonly when using a certain hashtag. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
