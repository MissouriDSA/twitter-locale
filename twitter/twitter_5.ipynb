{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter: An Analysis in Linguistic Diversity\n",
    "\n",
    "## Part V\n",
    "\n",
    "In the previous Twitter notebook, we left off exploring the time dimension of tweets and linguistic diversity. Now we can begin to look into the content of the tweets themselves. We will begin small (least complex) and eventually extend our analysis to more complex linguistic analytics. For the time being, we will be pausing our progression on linguistic diversity in order to grasp the basics of analyzing natural language. But don't worry, we will wrap back around to linguistic diversity and how it relates to some of the more advanced computational linguistic components.\n",
    "\n",
    "---\n",
    "\n",
    "### Retweets\n",
    "\n",
    "We will begin by looking at retweets. For those that are unfamiliar with the functionality of Twitter, a user has the ability to tweet someone else's tweet. This is (cleverly) known as a retweet. The reasons for \"retweeting\" are many, but there are certainly some information is encoded into a retweet, may this be aggreement with the tweet, popularity of the user tweeting, or some combination of the two.\n",
    "\n",
    "Retweets are easy to extract from this database. All retweets begin with \"RT\" in the `text` feature of the `tweet` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL FIRST!!!\n",
    "\n",
    "import time\n",
    "from pydrill.client import PyDrill\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pydrill**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "drill = PyDrill(host='128.206.116.250', port=8048)\n",
    "\n",
    "if not drill.is_active():\n",
    "    raise ImproperlyConfigured('Please run Drill first')\n",
    "\n",
    "\n",
    "# Start the Timer\n",
    "start = time.perf_counter()\n",
    "\n",
    "rows = drill.query(\"SELECT * FROM  dfs.datasets.`twitter/tweet.json` WHERE text LIKE 'RT%' LIMIT 100000\")\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for PyDrill:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "\n",
    "\n",
    "# pandas dataframe\n",
    "\n",
    "df = rows.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "statement = \"\"\"\n",
    "SELECT * \n",
    "FROM  twitter.tweet \n",
    "WHERE text LIKE 'RT%' \n",
    "LIMIT 100000;\n",
    "\"\"\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for Postgres:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "df = pd.DataFrame(job)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple enough. Just as a reminder, the `%` operator in SQL matches any or no characters.\n",
    "\n",
    "But we can also find some simple characteristics about retweets, such as the number of unique retweets..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pydrill**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT COUNT(*) \n",
    "FROM \n",
    "    (SELECT DISTINCT text\n",
    "    FROM \n",
    "        (SELECT text \n",
    "        FROM dfs.datasets.`twitter/tweet.json` \n",
    "        WHERE text LIKE 'RT%' \n",
    "        LIMIT 10000) AS retweets) AS retweet_count\"\"\"\n",
    "\n",
    "drill = PyDrill(host='128.206.116.250', port=8048)\n",
    "\n",
    "if not drill.is_active():\n",
    "    raise ImproperlyConfigured('Please run Drill first')\n",
    "\n",
    "\n",
    "# Start the Timer\n",
    "start = time.perf_counter()\n",
    "\n",
    "rows = drill.query(statement)\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for PyDrill:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "\n",
    "\n",
    "# pandas dataframe\n",
    "\n",
    "df = rows.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT COUNT(*) \n",
    "FROM \n",
    "    (SELECT DISTINCT text\n",
    "    FROM \n",
    "        (SELECT text \n",
    "        FROM twitter.tweet \n",
    "        WHERE text \n",
    "        LIKE 'RT%' \n",
    "        LIMIT 10000) AS retweets) AS retweet_count\"\"\"\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for Postgres:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "df = pd.DataFrame(job)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that `LIMIT` works independently of `ORDER BY` so the results of the queries may be slightly different each time you run it since we aren't taking into account all rows in the database.\n",
    "\n",
    "Again, counting the number of unique retweets isn't that interesting. What would be more interesting is see what retweets are the most popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT DISTINCT text, COUNT(*) \n",
    "FROM \n",
    "    (SELECT text \n",
    "    FROM twitter.tweet \n",
    "    WHERE text LIKE 'RT%' \n",
    "    LIMIT 10000) AS retweets \n",
    "GROUP BY text \n",
    "ORDER BY count DESC;\"\"\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for Postgres:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "df = pd.DataFrame(job)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this even further and circle back to the linguistic component. We can add another feature to the group by to find the count of the most popular retweets per language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT DISTINCT text, iso_language, COUNT(*) \n",
    "FROM \n",
    "    (SELECT text, iso_language \n",
    "    FROM twitter.tweet \n",
    "    WHERE text LIKE 'RT%' \n",
    "    LIMIT 10000) AS retweets \n",
    "GROUP BY text, iso_language \n",
    "ORDER BY count DESC;\"\"\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for Postgres:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "df = pd.DataFrame(job)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT DISTINCT job_id, text, iso_language, COUNT(*) \n",
    "FROM \n",
    "    (SELECT job_id, text, iso_language \n",
    "    FROM twitter.tweet \n",
    "    WHERE text LIKE 'RT%' AND job_id >= 255 \n",
    "    LIMIT 10000) AS retweets \n",
    "GROUP BY job_id, text, iso_language \n",
    "ORDER BY count DESC;\"\"\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for Postgres:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "df = pd.DataFrame(job)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further limit our query by a certain hashtag. It may be of interest to find the most common retweet by a certain topic where a topic is defined by a tweet hashtag. We have been collecting data for the duration of the 2017 MLB season, so if you are a baseball fan, we can extract the most retweeted tweets for a particular baseball team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT DISTINCT job_id, text, iso_language, COUNT(*) \n",
    "FROM \n",
    "    (SELECT job_id, text, iso_language \n",
    "    FROM twitter.tweet \n",
    "    WHERE lower(text) \n",
    "    LIKE 'rt%#royals%' AND job_id >= 255 LIMIT 100) AS retweets \n",
    "GROUP BY job_id, text, iso_language \n",
    "ORDER BY count DESC;\"\"\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for Postgres:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "df = pd.DataFrame(job)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that there were use `lower(text)` in the query. This turns all characters to lowercase as we don't want to mess with cases when looking for a simple hashtag.\n",
    "\n",
    "If you are unfamiliar with the Royals, they are a team from Kansas City, MO and there seems to be a lot of retweets coming from job_id 269. If you had to guess, where might this tweet come from? In fact, we can find that fairly simply by joining with the `job` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT DISTINCT description, iso_language, COUNT(*) \n",
    "FROM \n",
    "    (SELECT h.description, t.job_id, t.text, t.iso_language \n",
    "    FROM twitter.tweet t, twitter.job h\n",
    "    WHERE lower(t.text) LIKE 'rt%#royals%' AND h.job_id >= 255 and h.job_id = t.job_id LIMIT 100) AS retweets \n",
    "GROUP BY  description, iso_language \n",
    "ORDER BY count DESC;\"\"\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "    # use our connection values to establish a connection\n",
    "    conn = psycopg2.connect(connect_str)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(statement)\n",
    "    \n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Uh oh, can't connect. Invalid dbname, user or password?\")\n",
    "    print(e)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# How long did this look up take?\n",
    "print(\"Time to for Postgres:\")\n",
    "print(end - start)\n",
    "print('------------------')\n",
    "\n",
    "# create dictionary from the rows and column names   \n",
    "job = {}\n",
    "for i in list(range(len(column_names))):\n",
    "     job['{}'.format(column_names[i])] = [x[i] for x in rows]\n",
    "\n",
    "# turn dictionary into a data frame\n",
    "df = pd.DataFrame(job)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't collecting on Kansas City proper, but Overland park is a suburb of Kansas City and the database is collecting on it. It makes sense that the majority of the tweets come from there. However, this can lead to some interesting questions that we may want to consider a time dimension for. Why are there other cities using \"#royals\"? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
